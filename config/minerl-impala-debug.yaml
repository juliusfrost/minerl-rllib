# use this config with $ python rllib_train.py -f config/minerl-impala-debug.yaml
# see Experiment.spec for valid config options:
# https://docs.ray.io/en/master/_modules/ray/tune/experiment.html#Experiment
minerl-impala-debug: # name of the experiment - name of the folder in results
    run: IMPALA # RL Algorithm
    env: MineRLRandomDebug-v0 # MineRL Environment
    stop:
        time_total_s: 169200 # two days minus one hour in seconds
        info/num_steps_sampled: 8000000 # competition sample limit

    # check point file located results/experiment-name/unique-name/checkpoint_X/checkpoint-X
    checkpoint_freq: 1
    checkpoint_at_end: true
    keep_checkpoints_num: 1 # keep the best checkpoint around
    checkpoint_score_attr: episode_reward_mean
    # restore: path_to_checkpoint_file # does not restore the replay buffer so use with caution

    local_dir: results # specifies the results folder
    num_samples: 1 # number of random seeds to try
    log_to_file: log.txt # useful for debugging with print statements, errors are logged to error.txt

    # config specific to algorithm
    # see rllib documentation: https://docs.ray.io/en/master/rllib-training.html#common-parameters
    config:
        rollout_fragment_length: 50
        train_batch_size: 500
        num_workers: 2
        num_envs_per_worker: 1
        num_gpus: 0 # set to 0 if cpu
        clip_rewards: True
        lr_schedule: [
            [0, 0.0001],
            [20000000, 0.000000000001],
        ]

        framework: torch
        model:
            custom_model: minerl_torch_model
            # custom_model_config: # use this to add any custom model configuration
