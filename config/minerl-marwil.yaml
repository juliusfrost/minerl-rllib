# use this config with $ python rllib_train.py -f config/minerl-impala-debug.yaml
# see Experiment.spec for valid config options:
# https://docs.ray.io/en/master/_modules/ray/tune/experiment.html#Experiment
minerl-marwil: # name of the experiment - name of the folder in results
    run: MARWIL # RL Algorithm
    env: MineRLObtainDiamondVectorObf-v0 # MineRL Environment
    stop:
        time_total_s: 169200 # two days minus one hour in seconds
        info/num_steps_sampled: 8000000 # competition sample limit

    # check point file located results/experiment-name/unique-name/checkpoint_X/checkpoint-X
    checkpoint_freq: 1
    checkpoint_at_end: true
    keep_checkpoints_num: 1 # keep the best checkpoint around
    checkpoint_score_attr: episode_reward_mean
    # restore: path_to_checkpoint_file # does not restore the replay buffer so use with caution

    local_dir: results # specifies the results folder
    num_samples: 1 # number of random seeds to try
    log_to_file: log.txt # useful for debugging with print statements, errors are logged to error.txt

    # config specific to algorithm
    # see rllib documentation: https://docs.ray.io/en/master/rllib-training.html#common-parameters
    config:
        framework: torch
        model:
            custom_model: minerl_torch_model
            # custom_model_config: # use this to add any custom model configuration
        # input data must be absolute path
        # input: path_to_json_files
        # In order to evaluate on an actual environment, use these following
        # settings:
#        evaluation_num_workers: 1
#        evaluation_interval: 1
#        evaluation_config:
#            input: sampler
        # Compare IL (beta=0) vs MARWIL.
#        beta:
#            grid_search: [0, 1]