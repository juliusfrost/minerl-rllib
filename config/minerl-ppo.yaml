minerl-ppo:
    run: PPO
    config:
        lambda: 0.95
        kl_coeff: 0.5
        clip_rewards: True
        clip_param: 0.1
        vf_clip_param: 10.0
        entropy_coeff: 0.01
        train_batch_size: 5000
        rollout_fragment_length: 100
        sgd_minibatch_size: 500
        num_sgd_iter: 10
        num_workers: 5
        num_envs_per_worker: 1
        batch_mode: truncate_episodes
        observation_filter: NoFilter
        vf_share_layers: true
        num_gpus: 1
        model:
            # Filter config. List of [out_channels, kernel, stride] for each filter
            conv_filters: [
            [64, [4, 4], 4],
            [128, [4, 4], 4],
            [256, [4, 4], 4],
            ]
            conv_activation: relu
            fcnet_activation: tanh
            fcnet_hiddens: [256]
            free_log_std: false
            no_final_linear: false
            vf_share_layers: true
            use_lstm: true
            max_seq_len: 20
            lstm_cell_size: 256
            lstm_use_prev_action_reward: true
